var documenterSearchIndex = {"docs":
[{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = EnergySamplers","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [EnergySamplers]","category":"page"},{"location":"reference/#EnergySamplers.AbstractSampler","page":"Reference","title":"EnergySamplers.AbstractSampler","text":"Base type for samplers.\n\n\n\n\n\n","category":"type"},{"location":"reference/#EnergySamplers.AbstractSampler-Tuple{Any, AbstractSamplingRule}","page":"Reference","title":"EnergySamplers.AbstractSampler","text":"(sampler::AbstractSampler)(\n    model,\n    rule::AbstractSamplingRule;\n    niter::Int = 100,\n    clip_grads::Union{Nothing,AbstractFloat} = 1e-2,\n    n_samples::Union{Nothing,Int} = nothing,\n    kwargs...,\n)\n\nSampling method for AbstractSampler. This method generates samples from the model's learned distribution. \n\nArguments\n\nsampler::AbstractSampler: The sampler to use.\nmodel: The model to sample from.\nrule::AbstractSamplingRule: The sampling rule to use.\nniter::Int=100: The number of iterations to perform.\nclip_grads::Union{Nothing,AbstractFloat}=nothing: The value to clip the gradients. This is useful to prevent exploding gradients when training joint energy models. If nothing, no clipping is performed.\nn_samples::Union{Nothing,Int}=nothing: The number of samples to generate.\nkwargs...: Additional keyword arguments.\n\nReturns\n\ninput_samples: The samples generated by the sampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.AbstractSamplingRule","page":"Reference","title":"EnergySamplers.AbstractSamplingRule","text":"Base type for sampling rules.\n\n\n\n\n\n","category":"type"},{"location":"reference/#EnergySamplers.ConditionalSampler","page":"Reference","title":"EnergySamplers.ConditionalSampler","text":"ConditionalSampler <: AbstractSampler\n\nGenerates conditional samples: x sim p(xy)\n\n\n\n\n\n","category":"type"},{"location":"reference/#EnergySamplers.ConditionalSampler-Tuple{Distributions.Distribution, Distributions.Distribution}","page":"Reference","title":"EnergySamplers.ConditionalSampler","text":"ConditionalSampler(\n    𝒟x::Distribution, 𝒟y::Distribution;\n    input_size::Dims, batch_size::Int,\n    max_len::Int=10000, prob_buffer::AbstractFloat=0.95\n)\n\nOuter constructor for ConditionalSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.ImproperSGLD","page":"Reference","title":"EnergySamplers.ImproperSGLD","text":"ImproperSGLD(α::Real=2.0, σ::Real=0.01)\n\nImproper SGLD optimizer.\n\nExamples\n\nopt = ImproperSGLD()\n\n\n\n\n\n","category":"type"},{"location":"reference/#EnergySamplers.JointSampler","page":"Reference","title":"EnergySamplers.JointSampler","text":"JointSampler <: AbstractSampler\n\nGenerates unconditional samples by drawing directly from joint distribution: x sim p(x y)\n\n\n\n\n\n","category":"type"},{"location":"reference/#EnergySamplers.JointSampler-Tuple{Distributions.Distribution, Distributions.Distribution}","page":"Reference","title":"EnergySamplers.JointSampler","text":"JointSampler(\n    𝒟x::Distribution, 𝒟y::Distribution, input_size::Dims, batch_size::Int;\n    max_len::Int=10000, prob_buffer::AbstractFloat=0.95\n)\n\nOuter constructor for JointSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.SGLD","page":"Reference","title":"EnergySamplers.SGLD","text":"SGLD(a::Real=1.0, b::Real=1.0, γ::Real=0.5)\n\nStochastic Gradient Langevin Dynamics (SGLD) optimizer.\n\nExamples\n\nopt = SGLD()\nopt = SGLD(2.0, 100.0, 0.9)\n\n\n\n\n\n","category":"type"},{"location":"reference/#EnergySamplers.UnconditionalSampler","page":"Reference","title":"EnergySamplers.UnconditionalSampler","text":"UnonditionalSampler <: AbstractSampler\n\nGenerates unconditional samples: x sim p(x)\n\n\n\n\n\n","category":"type"},{"location":"reference/#EnergySamplers.UnconditionalSampler-Tuple{Distributions.Distribution, Union{Nothing, Distributions.Distribution}}","page":"Reference","title":"EnergySamplers.UnconditionalSampler","text":"UnconditionalSampler(\n    𝒟x::Distribution,\n    𝒟y::Union{Distribution,Nothing};\n    input_size::Dims,\n    batch_size::Int = 1,\n    max_len::Int = 10000,\n    prob_buffer::AbstractFloat = 0.95,\n)\n\nOuter constructor for UnonditionalSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.PMC-Tuple{AbstractSampler, Any, AbstractSamplingRule}","page":"Reference","title":"EnergySamplers.PMC","text":"PMC(\n    sampler::AbstractSampler,\n    model,\n    rule::AbstractSamplingRule;\n    ntransitions::Int = 100,\n    niter::Int = 100,\n    kwargs...,\n)\n\nRuns a Persistent Markov Chain (PMC) using the sampler and model. Persistent Markov Chains are used, for example, for Persistent Contrastive Convergence (Tieleman (2008)), a variant of the Contrastive Divergence (CD) algorithm. The main difference is that PCD uses a persistent chain to estimate the negative phase of the gradient. This is done by keeping the state of the Markov chain between iterations. \n\nIn our context, the sampler is the persistent chain and the model is a supervised model. The sampler generates samples from the model's learned distribution. \n\nNote\n\nThis function does not perform any training. It only generates samples from the model. In other words, there is no Contrastive Divergence. For training Joint Energy Models, see JointEnergyModels.jl.\n\nArguments\n\nsampler::AbstractSampler: The sampler to use.\nmodel: The model to sample from.\nrule::AbstractSamplingRule: The sampling rule to use.\nntransitions::Int=100: The number of transitions to perform.\nniter::Int=100: The number of iterations to perform.\nkwargs...: Additional keyword arguments.\n\nReturns\n\nsampler.buffer: The buffer containing the samples generated by the sampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers._energy-Tuple{Any, Any, Int64}","page":"Reference","title":"EnergySamplers._energy","text":"_energy(f, x, y::Int; agg=mean)\n\nComputes the energy for conditional samples x sim p_theta(xy): E(x)=- f_theta(x)y. Here f is the model, x is the input and y is the index of the target label. DOC_Grathwohl \n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers._energy-Tuple{Any, Any}","page":"Reference","title":"EnergySamplers._energy","text":"_energy(f, x; agg=mean)\n\nComputes the energy for unconditional samples x sim p_theta(x): E(x)=-textLogSumExp_y f_theta(x)y. DOC_Grathwohl\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.energy-Tuple{ConditionalSampler, Any, Any, Any}","page":"Reference","title":"EnergySamplers.energy","text":"energy(sampler::ConditionalSampler, model, x, y)\n\nEnergy function for ConditionalSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.energy-Tuple{JointSampler, Any, Any, Any}","page":"Reference","title":"EnergySamplers.energy","text":"energy(sampler::JointSampler, model, x, y)\n\nEnergy function for JointSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.energy-Tuple{UnconditionalSampler, Any, Any, Any}","page":"Reference","title":"EnergySamplers.energy","text":"energy(sampler::UnconditionalSampler, model, x, y)\n\nEnergy function for UnconditionalSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.energy_differential-Tuple{Any, Any, Any, Int64}","page":"Reference","title":"EnergySamplers.energy_differential","text":"energy_differential(f, xgen, xsampled, y::Int; agg=mean)\n\nComputes the energy differential between a conditional sample x_textgen sim p_theta(xy) and an observed sample x_textsample sim p(xy) as E(x_textsampley) - E(x_textgeny) with E(xy) = -f_theta(x)y. Here f is the model, xgen are the generated samples, xsampled are the observed training samples and y is the index of the target label. DOC_Grathwohl\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.energy_penalty-Tuple{Any, Any, Any, Int64}","page":"Reference","title":"EnergySamplers.energy_penalty","text":"energy_penalty(f, xgen, xsampled, y::Int; agg=mean)\n\nComputes the a Ridge penalty for the overall energies of the conditional samples x_textgen sim p_theta(xy) and an observed sample x_textsample sim p(xy). Here f is the model, xgen are the generated samples, xsampled are the observed training samples and y is the index of the target label. DOC_Grathwohl\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.get_logits-Tuple{Flux.Chain, Any}","page":"Reference","title":"EnergySamplers.get_logits","text":"get_logits(f::Flux.Chain, x)\n\nRetrieves the logits (linear predictions) of a Chain for the input x.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.mcmc_samples-Tuple{ConditionalSampler, Any, Optimisers.AbstractRule, AbstractArray}","page":"Reference","title":"EnergySamplers.mcmc_samples","text":"mcmc_samples(\n    sampler::ConditionalSampler,\n    model,\n    rule::Optimisers.AbstractRule,\n    input_samples::AbstractArray;\n    niter::Int,\n    y::Union{Nothing,Int} = nothing,\n)\n\nSampling method for ConditionalSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.mcmc_samples-Tuple{JointSampler, Any, Optimisers.AbstractRule, AbstractArray}","page":"Reference","title":"EnergySamplers.mcmc_samples","text":"mcmc_samples(\n    sampler::JointSampler,\n    model,\n    rule::Optimisers.AbstractRule,\n    input_samples::AbstractArray;\n    niter::Int,\n    y::Union{Nothing,Int} = nothing,\n)\n\nSampling method for JointSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#EnergySamplers.mcmc_samples-Tuple{UnconditionalSampler, Any, Optimisers.AbstractRule, AbstractArray}","page":"Reference","title":"EnergySamplers.mcmc_samples","text":"mcmc_samples(\n    sampler::UnconditionalSampler,\n    model,\n    rule::Optimisers.AbstractRule,\n    input_samples::AbstractArray;\n    niter::Int,\n    y::Union{Nothing,Int} = nothing,\n)\n\nSampling method for UnconditionalSampler.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = EnergySamplers","category":"page"},{"location":"#EnergySamplers","page":"Home","title":"EnergySamplers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for EnergySamplers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"EnergySamplers.jl is a small and lightweight package for sampling from probability distributions using methods from energy-based modelling (EBM). Its functionality is used in other Taija packages, including JointEnergyModels.jl and CounterfactualExplanations.jl.","category":"page"},{"location":"#Extensions-to-[Optimisers.jl](https://fluxml.ai/Optimisers.jl/stable/)","page":"Home","title":"Extensions to Optimisers.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package adds two new optimisers that are compatible with the Optimisers.jl interface:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Stochastic Gradient Langevin Dynamics (SGLD) (Welling and Teh 2011) — SGLD.\nImproper SGLD (see, for example, Grathwohl et al. (2020)) — ImproperSGLD.","category":"page"},{"location":"","page":"Home","title":"Home","text":"SGLD is an efficient gradient-based Markov Chain Monte Carlo (MCMC) method that can be used in the context of EBM to draw samples from the model posterior (Murphy 2023). Formally, we can draw from p_theta(x) as follows","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\n    x_j+1 leftarrow x_j - fracepsilon_j^22 nabla_x mathcalE_theta(x_j) + epsilon_j r_j  j=1J\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where r_j sim mathcalN(0I) is a stochastic term and the step-size epsilon_j is typically polynomially decayed (Welling and Teh 2011). To allow for faster sampling, it is common practice to choose the step-size epsilon_j and the standard deviation of r_j separately. While x_J is only guaranteed to distribute as p_theta(x) if epsilon rightarrow 0 and J rightarrow infty, the bias introduced for a small finite epsilon is negligible in practice (Murphy 2023). We denote this form of sampling as Improper SGLD.","category":"page"},{"location":"#Example:-Bayesian-Inferecne-with-SGLD","page":"Home","title":"Example: Bayesian Inferecne with SGLD","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To illustrate how the custom optimisers can be used, we will go through an example adapted from this (great!) blog post by Sebastian Callh. First, let’s load some dependencies:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# External dependencies:\nusing Flux\nusing Flux: gpu\nusing MLDataUtils: shuffleobs, stratifiedobs, rescale!\nusing Plots\nusing Random\nusing RDatasets \nusing Statistics\n# Custom optimisers:\nusing EnergySamplers: ImproperSGLD, SGLD","category":"page"},{"location":"","page":"Home","title":"Home","text":"Next, we load some data and prepare it for training a logistic regression model in Flux.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Random.seed!(2024)\n\ndata = dataset(\"ISLR\", \"Default\")\ntodigit(x) = x == \"Yes\" ? 1.0 : 0.0\ndata[!, :Default] = map(todigit, data[:, :Default])\ndata[!, :Student] = map(todigit, data[:, :Student])\n\ntarget = :Default\nnumerics = [:Balance, :Income]\nfeatures = [:Student, :Balance, :Income]\ntrain, test = (d -> stratifiedobs(first, d; p=0.7))(shuffleobs(data))\n\nfor feature in numerics\n    μ, σ = rescale!(train[!, feature]; obsdim=1)\n    rescale!(test[!, feature], μ, σ; obsdim=1)\nend\n\nprep_X(x) = Matrix(x)'\nprep_y(y) = reshape(y, 1, :)\ntrain_X, test_X = prep_X.((train[:, features], test[:, features]))\ntrain_y, test_y = prep_y.((train[:, target], test[:, target]))\ntrain_set = Flux.DataLoader((train_X, train_y); batchsize=100, shuffle=false)","category":"page"},{"location":"","page":"Home","title":"Home","text":"70-element DataLoader(::Tuple{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}, Matrix{Float64}}, batchsize=100)\n  with first element:\n  (3×100 Matrix{Float64}, 1×100 Matrix{Float64},)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, we create a small helper function that runs the training loop for a given optimiser opt and number of steps:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function train_logreg(; steps::Int=1000, opt=Flux.Descent(2))\n    Random.seed!(1)\n\n    paramvec(θ) = reduce(hcat, θ)\n    model = Dense(length(features), 1, sigmoid)\n    θ = Flux.params(model)\n    θ₀ = paramvec(θ)\n\n    predict(x; thres=0.5) = model(x) .> thres\n    accuracy(x, y) = mean(predict(x) .== y)\n\n    loss(yhat, y) = Flux.binarycrossentropy(yhat, y)\n    avg_loss(yhat, y) = mean(loss(yhat, y))\n    trainloss() = avg_loss(model(train_X), train_y)\n    testloss() = avg_loss(model(test_X), test_y)\n\n    trainlosses = [trainloss(); zeros(steps)]\n    testlosses = [testloss(); zeros(steps)]\n    weights = [θ₀; zeros(steps, length(θ₀))]\n\n    opt_state = Flux.setup(opt, model)\n\n    for t in 1:steps\n        for data in train_set\n            input, label = data\n\n            # Calculate the gradient of the objective\n            # with respect to the parameters within the model:\n            grads = Flux.gradient(model) do m\n                result = m(input)\n                loss(result, label)\n            end\n\n            Flux.update!(opt_state, model, grads[1])\n        end\n\n        # Bookkeeping\n        weights[t + 1, :] = paramvec(θ)\n        trainlosses[t + 1] = trainloss()\n        testlosses[t + 1] = testloss()\n    end\n\n    println(\"Final parameters are $(paramvec(θ))\")\n    println(\"Test accuracy is $(accuracy(test_X, test_y))\")\n\n    return model, weights, trainlosses, testlosses\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"train_logreg (generic function with 1 method)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now we use this function to train the model, first using SGLD and then using Improper SGLD:","category":"page"},{"location":"","page":"Home","title":"Home","text":"results = train_logreg(; steps=100, opt=SGLD(10.0, 10.0, 0.9))\nmodel, weights, trainlosses, testlosses = results\np1 = plot(weights; label=[\"Student\" \"Balance\" \"Income\" \"Intercept\"], plot_title=\"SGLD\")\n\nresults = train_logreg(; steps=100, opt=ImproperSGLD(2.0, 0.01))\nmodel, weights, trainlosses, testlosses = results\np2 = plot(weights; label=[\"Student\" \"Balance\" \"Income\" \"Intercept\"], plot_title=\"Improper SGLD\")\n\nplot(p1, p2, size=(800, 400))","category":"page"},{"location":"","page":"Home","title":"Home","text":"┌ Warning: `Flux.params(m...)` is deprecated. Use `Flux.trainable(model)` for parameter collection,\n│ and the explicit `gradient(m -> loss(m, x, y), model)` for gradient computation.\n└ @ Flux ~/.julia/packages/Flux/Mhg1r/src/deprecations.jl:93\n┌ Warning: Layer with Float32 parameters got Float64 input.\n│   The input will be converted, but any earlier layers may be very slow.\n│   layer = Dense(3 => 1, σ)    # 4 parameters\n│   summary(x) = \"3×7000 adjoint(::Matrix{Float64}) with eltype Float64\"\n└ @ Flux ~/.julia/packages/Flux/Mhg1r/src/layers/stateless.jl:60\nFinal parameters are Float32[-2.3311744 1.1305944 -1.5102222 -4.0762844]\nTest accuracy is 0.9666666666666667\nFinal parameters are Float32[-0.6106307 2.760134 -0.031244753 -5.8856964]\nTest accuracy is 0.9763333333333334","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#Energy-Based-Samplers","page":"Home","title":"Energy-Based Samplers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the context of EBM, the optimisers can be used to sample from a model posterior. To this end, the package provides the following samples:","category":"page"},{"location":"","page":"Home","title":"Home","text":"UnconditionalSampler — samples from the unconditional distribution p_theta(x) as in Grathwohl et al. (2020).\nConditionalSampler — samples from the conditional distribution p_theta(xy) as in Grathwohl et al. (2020).\nJointSampler — samples from the joint distribution p_theta(xy) as in Kelly, Zemel, and Grathwohl (2021).","category":"page"},{"location":"#Example:-Joint-Energy-Based-Model","page":"Home","title":"Example: Joint Energy-Based Model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The conditional sampler is used to draw class-conditional samples from a joint energy-based model (JEM) trained using Taija’s JointEnergyModels.jl. JEMs are explicitly trained to not only discriminate between output classes but also generate inputs. Hence, in the image below we can see that the model’s posterior conditional distributions (both over outputs and inputs) seem to approximate the true underlying distributions reasonably well: the model has learned to discriminate between the two classes (as indicated by the contours) and to generate samples from each class (as indicated by the stars).","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#Worked-Example","page":"Home","title":"Worked Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Next, we will present a simple worked example involving linearly separable Gaussian blobs:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Distributions\nusing MLJBase\n\n# Data:\nnobs = 2000\nX, y = make_blobs(nobs; centers=2, center_box=(-2. => 2.), cluster_std=0.1)\nXmat = Float32.(permutedims(matrix(X)))\nX = table(permutedims(Xmat))\nbatch_size = Int(round(nobs / 10))\n\n# Distributions:\n𝒟x = Normal()\n𝒟y = Categorical(ones(2) ./ 2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distributions.Categorical{Float64, Vector{Float64}}(support=Base.OneTo(2), p=[0.5, 0.5])","category":"page"},{"location":"","page":"Home","title":"Home","text":"We train a simple linear classifier to discriminate between output classes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Train a simple neural network on the data (classification)\nXtrain = permutedims(MLJBase.matrix(X))\nytrain = Flux.onehotbatch(y, levels(y))\ntrain_set = zip(eachcol(Xtrain), eachcol(ytrain))\ninputdim = size(first(train_set)[1], 1)\noutputdim = size(first(train_set)[2], 1)\nnn = Chain(Dense(inputdim, outputdim))\nloss(yhat, y) = Flux.logitcrossentropy(yhat, y)\nopt_state = Flux.setup(Flux.Adam(), nn)\nepochs = 5\nfor epoch in 1:epochs\n    Flux.train!(nn, train_set, opt_state) do m, x, y\n        loss(m(x), y)\n    end\n    @info \"Epoch $epoch\"\n    println(\"Accuracy: \", mean(Flux.onecold(nn(Xtrain)) .== Flux.onecold(ytrain)))\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"[ Info: Epoch 1\nAccuracy: 0.9995\n[ Info: Epoch 2\nAccuracy: 0.9995\n[ Info: Epoch 3\nAccuracy: 0.9995\n[ Info: Epoch 4\nAccuracy: 0.9995\n[ Info: Epoch 5\nAccuracy: 0.9995","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, we draw conditional samples from the model. Since we used a purely discriminative model for the task, the estimated posterior conditional distributions over inputs are not very good: the conditionally drawn samples (Xhat) largely lie on the right side of the decision boundary, but they are clearly not generated by the same data generating process as the training data.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using EnergySamplers: ConditionalSampler, PMC\n\n# PMC\nbs = 10\nntrans = 10\nniter = 100\n# Conditionally sample from first class:\nsmpler = ConditionalSampler(\n    𝒟x, 𝒟y; input_size=size(Xmat)[1:(end - 1)], batch_size=bs\n)\nx1 = PMC(smpler, nn, ImproperSGLD(); ntransitions=ntrans, niter=niter, y=1)\n# Conditionally sample from second class:\nsmpler = ConditionalSampler(\n    𝒟x, 𝒟y; input_size=size(Xmat)[1:(end - 1)], batch_size=bs\n)\nx2 = PMC(smpler, nn, ImproperSGLD(); ntransitions=ntrans, niter=niter, y=2)\n\n# Contour plot for predictions:\nxlims = extrema(hcat(x1,x2)[1,:]) .* 1.1\nylims = extrema(hcat(x1,x2)[2,:]) .* 1.1\nxrange = range(xlims[1], xlims[2], 100)\nyrange = range(ylims[1], ylims[2], 100)\nz = [softmax(nn([x, y])) for x in xrange, y in yrange] |> z -> reduce(hcat, z)\nplt = contourf(xrange, yrange, z[1,:], lw=0.1, xlims=xlims, ylims=ylims)\n\n# Plot samples:\nscatter!(Xtrain[1, :], Xtrain[2, :], color=Int.(y.refs), group=Int.(y.refs), label=[\"X|y=0\" \"X|y=1\"], ms=2, markerstrokecolor=Int.(y.refs))\nscatter!(x1[1, :], x1[2, :], color=1, label=\"Xhat|y=0\", ms=4, alpha=0.5)\nscatter!(x2[1, :], x2[2, :], color=2, label=\"Xhat|y=1\", ms=4, alpha=0.5)\nplot(plt)","category":"page"},{"location":"","page":"Home","title":"Home","text":"┌ Warning: Layer with Float32 parameters got Float64 input.\n│   The input will be converted, but any earlier layers may be very slow.\n│   layer = Dense(2 => 2)       # 6 parameters\n│   summary(x) = \"2-element Vector{Float64}\"\n└ @ Flux ~/.julia/packages/Flux/Mhg1r/src/layers/stateless.jl:60","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. 2020. “Your Classifier Is Secretly an Energy Based Model and You Should Treat It Like One.” In International Conference on Learning Representations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Kelly, Jacob, Richard Zemel, and Will Grathwohl. 2021. “Directly Training Joint Energy-Based Models for Conditional Synthesis and Calibrated Prediction of Multi-Attribute Data.” https://arxiv.org/abs/2108.04227.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Murphy, Kevin P. 2023. Probabilistic Machine Learning: Advanced Topics. MIT press.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Welling, Max, and Yee W Teh. 2011. “Bayesian Learning via Stochastic Gradient Langevin Dynamics.” In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 681–88. Citeseer.","category":"page"}]
}
