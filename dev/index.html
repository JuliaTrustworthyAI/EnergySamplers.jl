<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home ¬∑ EnergySamplers.jl</title><meta name="title" content="Home ¬∑ EnergySamplers.jl"/><meta property="og:title" content="Home ¬∑ EnergySamplers.jl"/><meta property="twitter:title" content="Home ¬∑ EnergySamplers.jl"/><meta name="description" content="Documentation for EnergySamplers.jl."/><meta property="og:description" content="Documentation for EnergySamplers.jl."/><meta property="twitter:description" content="Documentation for EnergySamplers.jl."/><meta property="og:url" content="https://JuliaTrustworthyAI.github.io/EnergySamplers.jl/"/><meta property="twitter:url" content="https://JuliaTrustworthyAI.github.io/EnergySamplers.jl/"/><link rel="canonical" href="https://JuliaTrustworthyAI.github.io/EnergySamplers.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>EnergySamplers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Extensions-to-[Optimisers.jl](https://fluxml.ai/Optimisers.jl/stable/)"><span>Extensions to Optimisers.jl</span></a></li><li><a class="tocitem" href="#Energy-Based-Samplers"><span>Energy-Based Samplers</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTrustworthyAI/EnergySamplers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTrustworthyAI/EnergySamplers.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="EnergySamplers"><a class="docs-heading-anchor" href="#EnergySamplers">EnergySamplers</a><a id="EnergySamplers-1"></a><a class="docs-heading-anchor-permalink" href="#EnergySamplers" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/JuliaTrustworthyAI/EnergySamplers.jl">EnergySamplers</a>.</p><p><code>EnergySamplers.jl</code> is a small and lightweight package for sampling from probability distributions using methods from energy-based modelling (EBM). Its functionality is used in other <a href="https://www.taija.org/">Taija</a> packages, including <a href="https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl">JointEnergyModels.jl</a> and <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>.</p><h2 id="Extensions-to-[Optimisers.jl](https://fluxml.ai/Optimisers.jl/stable/)"><a class="docs-heading-anchor" href="#Extensions-to-[Optimisers.jl](https://fluxml.ai/Optimisers.jl/stable/)">Extensions to <a href="https://fluxml.ai/Optimisers.jl/stable/">Optimisers.jl</a></a><a id="Extensions-to-[Optimisers.jl](https://fluxml.ai/Optimisers.jl/stable/)-1"></a><a class="docs-heading-anchor-permalink" href="#Extensions-to-[Optimisers.jl](https://fluxml.ai/Optimisers.jl/stable/)" title="Permalink"></a></h2><p>The package adds two new optimisers that are compatible with the <a href="https://fluxml.ai/Optimisers.jl/stable/">Optimisers.jl</a> interface:</p><ol><li>Stochastic Gradient Langevin Dynamics (SGLD) (Welling and Teh 2011) ‚Äî <a href="reference/#EnergySamplers.SGLD"><code>SGLD</code></a>.</li><li>Improper SGLD (see, for example, Grathwohl et al. (2020)) ‚Äî <a href="reference/#EnergySamplers.ImproperSGLD"><code>ImproperSGLD</code></a>.</li></ol><p>SGLD is an efficient gradient-based Markov Chain Monte Carlo (MCMC) method that can be used in the context of EBM to draw samples from the model posterior (Murphy 2023). Formally, we can draw from <span>$p_{\theta}(x)$</span> as follows</p><p class="math-container">\[\begin{aligned}
    x_{j+1} &amp;\leftarrow x_j - \frac{\epsilon_j^2}{2} \nabla_x \mathcal{E}_{\theta}(x_j) + \epsilon_j r_j, &amp;&amp; j=1,...,J
\end{aligned}\]</p><p>where <span>$r_j \sim \mathcal{N}(0,I)$</span> is a stochastic term and the step-size <span>$\epsilon_j$</span> is typically polynomially decayed (Welling and Teh 2011). To allow for faster sampling, it is common practice to choose the step-size <span>$\epsilon_j$</span> and the standard deviation of <span>$r_j$</span> separately. While <span>$x_J$</span> is only guaranteed to distribute as <span>$p_{\theta}(x)$</span> if <span>$\epsilon \rightarrow 0$</span> and <span>$J \rightarrow \infty$</span>, the bias introduced for a small finite <span>$\epsilon$</span> is negligible in practice (Murphy 2023). We denote this form of sampling as Improper SGLD.</p><h3 id="Example:-Bayesian-Inferecne-with-SGLD"><a class="docs-heading-anchor" href="#Example:-Bayesian-Inferecne-with-SGLD">Example: Bayesian Inferecne with SGLD</a><a id="Example:-Bayesian-Inferecne-with-SGLD-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Bayesian-Inferecne-with-SGLD" title="Permalink"></a></h3><p>To illustrate how the custom optimisers can be used, we will go through an example adapted from this (great!) blog <a href="https://sebastiancallh.github.io/post/langevin/">post</a> by Sebastian Callh. First, let‚Äôs load some dependencies:</p><pre><code class="language-julia hljs"># External dependencies:
using Flux
using Flux: gpu
using MLDataUtils: shuffleobs, stratifiedobs, rescale!
using Plots
using Random
using RDatasets 
using Statistics
# Custom optimisers:
using EnergySamplers: ImproperSGLD, SGLD</code></pre><p>Next, we load some data and prepare it for training a logistic regression model in <a href="https://fluxml.ai/Flux.jl/stable/">Flux.jl</a>:</p><pre><code class="language-julia hljs">Random.seed!(2024)

data = dataset(&quot;ISLR&quot;, &quot;Default&quot;)
todigit(x) = x == &quot;Yes&quot; ? 1.0 : 0.0
data[!, :Default] = map(todigit, data[:, :Default])
data[!, :Student] = map(todigit, data[:, :Student])

target = :Default
numerics = [:Balance, :Income]
features = [:Student, :Balance, :Income]
train, test = (d -&gt; stratifiedobs(first, d; p=0.7))(shuffleobs(data))

for feature in numerics
    Œº, œÉ = rescale!(train[!, feature]; obsdim=1)
    rescale!(test[!, feature], Œº, œÉ; obsdim=1)
end

prep_X(x) = Matrix(x)&#39;
prep_y(y) = reshape(y, 1, :)
train_X, test_X = prep_X.((train[:, features], test[:, features]))
train_y, test_y = prep_y.((train[:, target], test[:, target]))
train_set = Flux.DataLoader((train_X, train_y); batchsize=100, shuffle=false)</code></pre><pre><code class="nohighlight hljs">70-element DataLoader(::Tuple{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}, Matrix{Float64}}, batchsize=100)
  with first element:
  (3√ó100 Matrix{Float64}, 1√ó100 Matrix{Float64},)</code></pre><p>Finally, we create a small helper function that runs the training loop for a given optimiser <code>opt</code> and number of <code>steps</code>:</p><pre><code class="language-julia hljs">function train_logreg(; steps::Int=1000, opt=Flux.Descent(2))
    Random.seed!(1)

    paramvec(Œ∏) = reduce(hcat, Œ∏)
    model = Dense(length(features), 1, sigmoid)
    Œ∏ = Flux.params(model)
    Œ∏‚ÇÄ = paramvec(Œ∏)

    predict(x; thres=0.5) = model(x) .&gt; thres
    accuracy(x, y) = mean(predict(x) .== y)

    loss(yhat, y) = Flux.binarycrossentropy(yhat, y)
    avg_loss(yhat, y) = mean(loss(yhat, y))
    trainloss() = avg_loss(model(train_X), train_y)
    testloss() = avg_loss(model(test_X), test_y)

    trainlosses = [trainloss(); zeros(steps)]
    testlosses = [testloss(); zeros(steps)]
    weights = [Œ∏‚ÇÄ; zeros(steps, length(Œ∏‚ÇÄ))]

    opt_state = Flux.setup(opt, model)

    for t in 1:steps
        for data in train_set
            input, label = data

            # Calculate the gradient of the objective
            # with respect to the parameters within the model:
            grads = Flux.gradient(model) do m
                result = m(input)
                loss(result, label)
            end

            Flux.update!(opt_state, model, grads[1])
        end

        # Bookkeeping
        weights[t + 1, :] = paramvec(Œ∏)
        trainlosses[t + 1] = trainloss()
        testlosses[t + 1] = testloss()
    end

    println(&quot;Final parameters are $(paramvec(Œ∏))&quot;)
    println(&quot;Test accuracy is $(accuracy(test_X, test_y))&quot;)

    return model, weights, trainlosses, testlosses
end</code></pre><pre><code class="nohighlight hljs">train_logreg (generic function with 1 method)</code></pre><p>Now we use this function to train the model, first using SGLD and then using Improper SGLD:</p><pre><code class="language-julia hljs">results = train_logreg(; steps=100, opt=SGLD(10.0, 10.0, 0.9))
model, weights, trainlosses, testlosses = results
p1 = plot(weights; label=[&quot;Student&quot; &quot;Balance&quot; &quot;Income&quot; &quot;Intercept&quot;], plot_title=&quot;SGLD&quot;)

results = train_logreg(; steps=100, opt=ImproperSGLD(2.0, 0.01))
model, weights, trainlosses, testlosses = results
p2 = plot(weights; label=[&quot;Student&quot; &quot;Balance&quot; &quot;Income&quot; &quot;Intercept&quot;], plot_title=&quot;Improper SGLD&quot;)

plot(p1, p2, size=(800, 400))</code></pre><pre><code class="nohighlight hljs">‚îå Warning: `Flux.params(m...)` is deprecated. Use `Flux.trainable(model)` for parameter collection,
‚îÇ and the explicit `gradient(m -&gt; loss(m, x, y), model)` for gradient computation.
‚îî @ Flux ~/.julia/packages/Flux/Mhg1r/src/deprecations.jl:93
‚îå Warning: Layer with Float32 parameters got Float64 input.
‚îÇ   The input will be converted, but any earlier layers may be very slow.
‚îÇ   layer = Dense(3 =&gt; 1, œÉ)    # 4 parameters
‚îÇ   summary(x) = &quot;3√ó7000 adjoint(::Matrix{Float64}) with eltype Float64&quot;
‚îî @ Flux ~/.julia/packages/Flux/Mhg1r/src/layers/stateless.jl:60
Final parameters are Float32[-2.3311744 1.1305944 -1.5102222 -4.0762844]
Test accuracy is 0.9666666666666667
Final parameters are Float32[-0.6106307 2.760134 -0.031244753 -5.8856964]
Test accuracy is 0.9763333333333334</code></pre><p><img src="index_files/figure-commonmark/cell-5-output-2.svg" alt/></p><h2 id="Energy-Based-Samplers"><a class="docs-heading-anchor" href="#Energy-Based-Samplers">Energy-Based Samplers</a><a id="Energy-Based-Samplers-1"></a><a class="docs-heading-anchor-permalink" href="#Energy-Based-Samplers" title="Permalink"></a></h2><p>In the context of EBM, the optimisers can be used to sample from a model posterior. To this end, the package provides the following samples:</p><ol><li><a href="reference/#EnergySamplers.UnconditionalSampler"><code>UnconditionalSampler</code></a> ‚Äî samples from the unconditional distribution <span>$p_{\theta}(x)$</span> as in Grathwohl et al. (2020).</li><li><a href="reference/#EnergySamplers.ConditionalSampler"><code>ConditionalSampler</code></a> ‚Äî samples from the conditional distribution <span>$p_{\theta}(x|y)$</span> as in Grathwohl et al. (2020).</li><li><a href="reference/#EnergySamplers.JointSampler"><code>JointSampler</code></a> ‚Äî samples from the joint distribution <span>$p_{\theta}(x,y)$</span> as in Kelly, Zemel, and Grathwohl (2021).</li></ol><h3 id="Example:-Joint-Energy-Based-Model"><a class="docs-heading-anchor" href="#Example:-Joint-Energy-Based-Model">Example: Joint Energy-Based Model</a><a id="Example:-Joint-Energy-Based-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Joint-Energy-Based-Model" title="Permalink"></a></h3><p>The conditional sampler is used to draw class-conditional samples from a joint energy-based model (JEM) trained using Taija‚Äôs <a href="https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl">JointEnergyModels.jl</a>. JEMs are explicitly trained to not only discriminate between output classes but also generate inputs. Hence, in the image below we can see that the model‚Äôs posterior conditional distributions (both over outputs and inputs) seem to approximate the true underlying distributions reasonably well: the model has learned to discriminate between the two classes (as indicated by the contours) and to generate samples from each class (as indicated by the stars).</p><p><img src="assets/jem.svg" alt/></p><h3 id="Worked-Example"><a class="docs-heading-anchor" href="#Worked-Example">Worked Example</a><a id="Worked-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Worked-Example" title="Permalink"></a></h3><p>Next, we will present a simple worked example involving linearly separable Gaussian blobs:</p><pre><code class="language-julia hljs">using Distributions
using MLJBase

# Data:
nobs = 2000
X, y = make_blobs(nobs; centers=2, center_box=(-2. =&gt; 2.), cluster_std=0.1)
Xmat = Float32.(permutedims(matrix(X)))
X = table(permutedims(Xmat))
batch_size = Int(round(nobs / 10))

# Distributions:
ùíüx = Normal()
ùíüy = Categorical(ones(2) ./ 2)</code></pre><pre><code class="nohighlight hljs">Distributions.Categorical{Float64, Vector{Float64}}(support=Base.OneTo(2), p=[0.5, 0.5])</code></pre><p>We train a simple linear classifier to discriminate between output classes:</p><pre><code class="language-julia hljs"># Train a simple neural network on the data (classification)
Xtrain = permutedims(MLJBase.matrix(X))
ytrain = Flux.onehotbatch(y, levels(y))
train_set = zip(eachcol(Xtrain), eachcol(ytrain))
inputdim = size(first(train_set)[1], 1)
outputdim = size(first(train_set)[2], 1)
nn = Chain(Dense(inputdim, outputdim))
loss(yhat, y) = Flux.logitcrossentropy(yhat, y)
opt_state = Flux.setup(Flux.Adam(), nn)
epochs = 5
for epoch in 1:epochs
    Flux.train!(nn, train_set, opt_state) do m, x, y
        loss(m(x), y)
    end
    @info &quot;Epoch $epoch&quot;
    println(&quot;Accuracy: &quot;, mean(Flux.onecold(nn(Xtrain)) .== Flux.onecold(ytrain)))
end</code></pre><pre><code class="nohighlight hljs">[ Info: Epoch 1
Accuracy: 0.9995
[ Info: Epoch 2
Accuracy: 0.9995
[ Info: Epoch 3
Accuracy: 0.9995
[ Info: Epoch 4
Accuracy: 0.9995
[ Info: Epoch 5
Accuracy: 0.9995</code></pre><p>Finally, we draw conditional samples from the model. Since we used a purely discriminative model for the task, the estimated posterior conditional distributions over inputs are not very good: the conditionally drawn samples (<code>Xhat</code>) largely lie on the right side of the decision boundary, but they are clearly not generated by the same data generating process as the training data.</p><pre><code class="language-julia hljs">using EnergySamplers: ConditionalSampler, PMC

# PMC
bs = 10
ntrans = 10
niter = 100
# Conditionally sample from first class:
smpler = ConditionalSampler(
    ùíüx, ùíüy; input_size=size(Xmat)[1:(end - 1)], batch_size=bs
)
x1 = PMC(smpler, nn, ImproperSGLD(); ntransitions=ntrans, niter=niter, y=1)
# Conditionally sample from second class:
smpler = ConditionalSampler(
    ùíüx, ùíüy; input_size=size(Xmat)[1:(end - 1)], batch_size=bs
)
x2 = PMC(smpler, nn, ImproperSGLD(); ntransitions=ntrans, niter=niter, y=2)

# Contour plot for predictions:
xlims = extrema(hcat(x1,x2)[1,:]) .* 1.1
ylims = extrema(hcat(x1,x2)[2,:]) .* 1.1
xrange = range(xlims[1], xlims[2], 100)
yrange = range(ylims[1], ylims[2], 100)
z = [softmax(nn([x, y])) for x in xrange, y in yrange] |&gt; z -&gt; reduce(hcat, z)
plt = contourf(xrange, yrange, z[1,:], lw=0.1, xlims=xlims, ylims=ylims)

# Plot samples:
scatter!(Xtrain[1, :], Xtrain[2, :], color=Int.(y.refs), group=Int.(y.refs), label=[&quot;X|y=0&quot; &quot;X|y=1&quot;], ms=2, markerstrokecolor=Int.(y.refs))
scatter!(x1[1, :], x1[2, :], color=1, label=&quot;Xhat|y=0&quot;, ms=4, alpha=0.5)
scatter!(x2[1, :], x2[2, :], color=2, label=&quot;Xhat|y=1&quot;, ms=4, alpha=0.5)
plot(plt)</code></pre><pre><code class="nohighlight hljs">‚îå Warning: Layer with Float32 parameters got Float64 input.
‚îÇ   The input will be converted, but any earlier layers may be very slow.
‚îÇ   layer = Dense(2 =&gt; 2)       # 6 parameters
‚îÇ   summary(x) = &quot;2-element Vector{Float64}&quot;
‚îî @ Flux ~/.julia/packages/Flux/Mhg1r/src/layers/stateless.jl:60</code></pre><p><img src="index_files/figure-commonmark/cell-8-output-2.svg" alt/></p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. 2020. ‚ÄúYour Classifier Is Secretly an Energy Based Model and You Should Treat It Like One.‚Äù In <em>International Conference on Learning Representations</em>.</p><p>Kelly, Jacob, Richard Zemel, and Will Grathwohl. 2021. ‚ÄúDirectly Training Joint Energy-Based Models for Conditional Synthesis and Calibrated Prediction of Multi-Attribute Data.‚Äù <a href="https://arxiv.org/abs/2108.04227">https://arxiv.org/abs/2108.04227</a>.</p><p>Murphy, Kevin P. 2023. <em>Probabilistic Machine Learning: Advanced Topics</em>. MIT press.</p><p>Welling, Max, and Yee W Teh. 2011. ‚ÄúBayesian Learning via Stochastic Gradient Langevin Dynamics.‚Äù In <em>Proceedings of the 28th International Conference on Machine Learning (ICML-11)</em>, 681‚Äì88. Citeseer.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="reference/">Reference ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 19 December 2024 13:37">Thursday 19 December 2024</span>. Using Julia version 1.10.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
